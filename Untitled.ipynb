{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing libraries...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f7071933dfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfirebase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfirebase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \t'''\n\u001b[1;32m     34\u001b[0m         \u001b[0mconvert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moperate\u001b[0m \u001b[0mon\u001b[0m \u001b[0minput\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f7071933dfed>\u001b[0m in \u001b[0;36mDocument\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;31m# visualize the lda space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mvis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vis_data' is not defined"
     ]
    }
   ],
   "source": [
    "print \"importing libraries...\"\n",
    "\n",
    "# saving and displaying output\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# text conversion \n",
    "from subprocess import Popen, PIPE\n",
    "from docx       import opendocx, getdocumenttext\n",
    "#http://stackoverflow.com/questions/5725278/python-help-using-pdfminer-as-a-library\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout    import LAParams\n",
    "from pdfminer.pdfpage   import PDFPage\n",
    "from cStringIO          import StringIO\n",
    "\n",
    "# proofreading by After The Deadline (https://blog.afterthedeadline.com/2009/09/15/python-bindings-for-atd/)\n",
    "import ATD\n",
    "\n",
    "# natural language processing\n",
    "from nltk        import data, tokenize, pos_tag\n",
    "from gensim      import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "\n",
    "# LDA visualization\t\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# database \n",
    "import json\n",
    "from firebase import firebase\n",
    "\n",
    "class Document():\n",
    "\t'''\n",
    "\tconvert, parse, and operate on input text\n",
    "\n",
    "\tTODO: \n",
    "\t- [ ] change all print statements to logs\n",
    "\t- [x] support for .pdf, .doc, .docx, and .odt\n",
    "\t- [x] support for .txt\n",
    "\t- [x] sentence tokenize\n",
    "\t- [ ] paragraph tokenize !!!\n",
    "\t- [x] part of speech tagging\n",
    "\t- [/] writing document to database -- what to include? everything?\n",
    "\t- [ ] LSA? lsa.colorado.edu?\n",
    "\t- [ ] word similarity matrix\n",
    "\t- [x] firebase\n",
    "\t- [ ] word2vec!!   https://radimrehurek.com/gensim/models/word2vec.html --> class gensim.models.word2vec.Word2Vec: Class for training, using and evaluating neural networks described in https://code.google.com/p/word2vec/\n",
    "\t- [ ] wikipedia dumps? (Already have AA through BC... might want to find precompiled set? takes many days to parse all pages from compressed archive)\n",
    "\t- [ ] bigram transformer (process phrases like words) !!\n",
    "\t- [ ] \n",
    "\t\t.\n",
    "\t\t.\n",
    "\t\t.\n",
    "\t'''\n",
    "\t# establish connection to firebase\n",
    "\tdatabase = firebase.FirebaseApplication('https://grademebaby.firebaseio.com/', None)\n",
    "\n",
    "\t# we might want to train the tokenizer on the format of the input text using PunktSentenceTokenizer(text)\n",
    "\t# these split the document into sentences\n",
    "\ttokenizer     = tokenize.TreebankWordTokenizer()\n",
    "\tsent_detector = data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\tdef __init__(self, filename):\n",
    "\t\tself.filename     = filename # contains extension\n",
    "\t\tself.raw          = str()    # document as str \n",
    "\t\tself.preprocessed = dict()   # text converted into ( word, lemma, [POS] ) format\n",
    "\t\tself.stats        = dict()\n",
    "\n",
    "\tdef convert_pdf_to_txt(self, path):\n",
    "\t\trsrcmgr     = PDFResourceManager()\n",
    "\t\tretstr      = StringIO()\n",
    "\t\tcodec       = 'utf-8'\n",
    "\t\tlaparams    = LAParams()\n",
    "\t\tdevice      = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "\t\tfp          = file(path, 'rb')\n",
    "\t\tinterpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\t\tpassword    = \"\"\n",
    "\t\tmaxpages    = 0\n",
    "\t\tcaching     = True\n",
    "\t\tpagenos     = set()\n",
    "\n",
    "\t\tmap(interpreter.process_page, [page for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True)])\n",
    "  \n",
    "  \t\tfp.close()\n",
    "    \t\tdevice.close()\n",
    "    \t\tstr = retstr.getvalue()\n",
    "    \t\tretstr.close()\n",
    "    \t\treturn str\n",
    "\n",
    "\tdef document_to_text(self, filename, file_path):\n",
    "\t\tif filename[-4:] == \".doc\":\n",
    "        \t\tcmd            = ['antiword', file_path]\n",
    "        \t\tp              = Popen(cmd, stdout=PIPE)\n",
    "        \t\tstdout, stderr = p.communicate()\n",
    "        \t\tself.raw       = stdout.decode('ascii', 'ignore')\n",
    "    \t\t\n",
    "\t\telif filename[-5:] == \".docx\":\n",
    "        \t\tdocument        = opendocx(file_path)\n",
    "        \t\tparatextlist    = getdocumenttext(document)\n",
    "        \t\tnewparatextlist = []\n",
    "        \t\tfor paratext in paratextlist:\n",
    "        \t\t\t newparatextlist.append(paratext.encode(\"utf-8\"))\n",
    "       \t\t\tself.raw = '\\n\\n'.join(newparatextlist)\n",
    "    \t\t\n",
    "\t\telif filename[-4:] == \".odt\":\n",
    "        \t\tcmd            = ['odt2txt', file_path]\n",
    "        \t\tp              = Popen(cmd, stdout=PIPE)\n",
    "        \t\tstdout, stderr = p.communicate()\n",
    "        \t\tself.raw       = stdout.decode('ascii', 'ignore')\n",
    "    \t\n",
    "\t\telif filename[-4:] == \".pdf\":\n",
    "        \t\tself.raw = self.convert_pdf_to_txt(file_path)\n",
    "\t\t\n",
    "\t\telif filename[-4:] == \".txt\":\n",
    "\t\t\twith open(file_path, 'r') as file_:\n",
    "\t\t\t\tself.raw = file_.read()\n",
    "\n",
    "\tdef proofread(self):\n",
    "\t\t # our API key for AfterTheDeadline\n",
    "\t\tATD.setDefaultKey(hash(\"DoubleCheck\")) \n",
    "\n",
    "\t\t# check the document for grammar and spelling errors \t\n",
    "\t\terrors = ATD.checkDocument(self.raw)\n",
    "\n",
    "\t\t'''\n",
    "\t\t# print the errors\n",
    "\t\tfor error in errors: \t\n",
    "\t\t\tprint \"%s error for: %s **%s**\" % (error.type, error.precontext, error.string)\n",
    "\t\t\tprint \"some suggestions: %s\" % (\", \".join(error.suggestions),)\n",
    "\t\t'''\n",
    "\n",
    "\t\t# write the errors to the database\n",
    "\t\terr2db = [{\"type\":        error.type,\n",
    "\t\t\t   \"precontext\":  error.precontext,\n",
    "\t\t\t   \"string\":      error.string,\n",
    "\t\t\t   \"suggestions\": error.suggestions} for error in errors] \n",
    "\n",
    "\t\tprint json.dumps(err2db, sort_keys=True, indent=4)\n",
    "\n",
    "\tdef vectorize(self):\n",
    "\t\t# tokenize and remove stopwords\n",
    "\t\tsentences = self.sent_detector.tokenize(self.raw.decode('utf-8').strip())\n",
    "\t\tstoplist  = set('for a of the and to in'.split())\n",
    "\t\ttexts     = [[word for word in sentence.lower().split() if word not in stoplist] for sentence in sentences]\n",
    "\t\t\n",
    "\t\t# compute the frequency of each token\n",
    "\t\tfrequency = defaultdict(int)\n",
    "\t\tfor text in texts:\n",
    "\t\t\tfor token in text:\n",
    "\t\t\t\tfrequency[token] += 1\n",
    "\n",
    "\t\t# remove words that appear only once\n",
    "\t\ttexts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\t\t\n",
    "\t\t# construct a gensim dictionary and corpus (bag of words)\n",
    "\t\tdictionary = corpora.Dictionary(texts)\n",
    "\t\tcorpus     = [dictionary.doc2bow(text) for text in texts] # currently, \"text\" is a sentence in the document\n",
    "\n",
    "\t\t# define LDA model\n",
    "\t\tlda = models.ldamodel.LdaModel( corpus       = corpus, \n",
    "\t\t\t\t\t\tid2word      = dictionary,\n",
    "\t\t\t\t\t\tnum_topics   = 100, #what should this be ???\n",
    "\t\t\t\t\t\tupdate_every = 1, \n",
    "\t\t\t\t\t\tchunksize    = 10000, \n",
    "\t\t\t\t\t\tpasses       = 100 )\n",
    "\t\t\n",
    "\t\t# print the extracted topics\n",
    "\t\tlda.print_topics(10)\t\n",
    "\n",
    "\t\t# visualize the lda space\n",
    "\t\t\n",
    "        vis_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "        pyLDAvis.display(vis_data)\n",
    "        \n",
    "\tdef preprocess_text(self):\n",
    "\t\tsentences = self.sent_detector.tokenize(self.raw.decode('utf-8').strip())\n",
    "\t\ttokens    = [self.tokenizer.tokenize(sentence) for sentence in sentences]\t\t\n",
    "\t\tpos       = [pos_tag(token) for token in tokens]\n",
    "\t\t\n",
    "\t\t# final format includes 1) token, 2) lemma, and 3) list of part of speech tags\t\t\n",
    "\t\tpos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\t\t\n",
    "\n",
    "\t\tself.preprocessed = { 'sentences': sentences,\n",
    "\t\t\t\t      'tokens'   : tokens,\n",
    "\t\t\t\t      'pos'      : pos }\n",
    "\n",
    "\tdef statistics(self):\n",
    "\t\tself.stats['sentences'] = len(self.preprocessed['sentences'])\n",
    "\t\tself.stats['tokens']    = len(self.preprocessed['tokens'])\n",
    "\n",
    "def main():\n",
    "\tuser     = raw_input('user: ')\n",
    "\tfilename = raw_input('filename: ') \n",
    "\tname     = filename[:-5] # this is wrong !!\n",
    "\n",
    "\tdoc = Document(filename)\n",
    "\n",
    "\t# check to see if the file is already in the database\n",
    "\tuser_files = doc.database.get('/documents/' + user, None)\n",
    "\tif name in user_files.keys():\n",
    "\t\t''' <-- currently broken?\n",
    "\t\tprint \"loading document from database...\"\n",
    "\t\tdoc.raw          = user_files[name].keys()[0]['raw'] # complicated syntax to bypass random key generation :(\n",
    "\t\tdoc.preprocessed = user_files[name].keys()[0]['tagged'] \n",
    "\t\tdoc.stats        = user_files[name].keys()[0]['stats']\n",
    "\t\t'''\n",
    "\telse:\n",
    "\t\tprint \"converting document to raw text...\"\n",
    "\t\tdoc.document_to_text(doc.filename, doc.filename)\n",
    "\t\tprint \"proofreading the document...\"\n",
    "\t\tdoc.proofread()\n",
    "\t\tprint \"vectorizing raw text and performing LDA...\"\n",
    "\t\tdoc.vectorize() # must be called after document_to_test\n",
    "\t\tprint \"NOT preprocessing raw text...\"\n",
    "\t\t#doc.preprocess_text()\n",
    "\t\tprint \"NOT getting document statistics...\"\n",
    "\t\t#doc.statistics()\n",
    "\t\tprint \"NOT writing document to database...\"\n",
    "\t\t'''\n",
    "\t\tdb_entry = { \"filename\": doc.filename,\n",
    "\t\t     \t     \"raw\":      doc.raw,\n",
    "\t\t     \t     \"tagged\":   doc.preprocessed,\n",
    "\t\t     \t     \"stats\":    doc.stats }\t\n",
    "\t\t'''\n",
    "\t\t#doc.database.post('/documents/' + user + \"/\" + name, db_entry) \t\n",
    "\n",
    "if __name__==\"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
